# Udacity_AWS_MLE_NanoDegree_Capstone
## This is the repository for the Capstone Project for the Udacity Scholarship AWS Machine Learning Engineer Nano Degree

NOTE: THIS PROJECT HAS BEEN REVIEWED AND APPROVED BY AN UDACITY NANODEGREE MENTOR

## Files and Folders in this Repo

- `proposal.pdf` is the Capstone Project Proposal 
- `Capstone_Project_Report.pdf` is the Capstone Project Report

### Data Related Notebooks

- `Create_Data_Capstone.ipynb` creates the Initial Dataset with `file_list.json` as a requirement
- `Get_More_Data_Script.ipynb` is for getting additional data to remove class imbalance -> This creates `New_Data` folder 
- `New_Data` Folder is used by `Create_Data_Capstone.ipynb` to balance the classes and create the Final Dataset to be used. 
- `file_list.json` is the initial meta data file provided by Udacity for getting a subset of the Amazon Bin Image Dataset

### Benchmark Related Notebooks
- `Benchmark_Model.ipynb` creates the Benchmarks for our Project
- `CNN_model_train_benchmark.py` is required by `Benchmark_Model.ipynb` for training Custom CNN for Benchmark in SageMaker
- `Resnet_model_train_benchmark.py` is required by `Benchmark_Model.ipynb` for finetuning Resnet for Benchmark in SageMaker


### Model Training, Evaluation and Deployment Related Notebooks
- `Final_Model_Train_Inference.ipynb` trains models in SageMaker to try to beat Benchmarks set by us
- `model_train.py` is required by `Final_Model_Train_Inference.ipynb` for training models in SageMaker
- `inference.py` is required by `Final_Model_Train_Inference.ipynb` for doing Inference from Endpoints in SageMaker

### Output Folders
- `Screen Shots` contains screenshot images required by Jupyter Notebooks in their markdowns 
- `Screen Shots\Hyper Parameter Training Jobs` contains screenshots of Hyper Parameter Training Jobs in SageMaker
- `profiler-output` contains the outputs and **Profiler Report** generated by SageMaker Debugger Profiler




# Inventory Monitoring at Distribution Centres

## Introduction to the Problem Domain and Statement

A lot of Corporations, which handle physical cargo and deal with supply chain of any kind of goods have tried to bring in automation to make their processes more efficient and accurate. A great example of this is Amazon, who is one of the biggest hubs of delivery of all kinds of goods. These goods are often stored in big warehouses. Since the quantity of these items are in a huge amount, to physically do inventory monitoring would require both large and intelligent human resources, which are both expensive and prone to errors. 

This is where robots come in to help in Inventory Monitoring. They can be trained with Machine Learning Models, to perform tasks like Object Detection,  Outlier & Anomaly Detection and much more. Once trained, these models are scalable, and can be deployed at a low cost for usage in actual warehouses and distribution centres on industry level robots. 

The robots carry objects which are present in bins, and for our problem, each bin can contain 1-5 objects.

The problem we aim to tackle in this project is to count the number of items present in the bin. This is an **Multi-Class Image Classification task**, of classifying number of items in 1 â€“ 5 in input image. This is a worthwhile problem to solve, for it has immense real world applications. If we can develop a model, which can take in a picture of a bin, and accurately return the number of objects present in that, we could solve & thus fully automate one crucial step in the Inventory Management process! 


## Dataset

### Overview

The dataset used in this problem is the open source Amazon Bin Image Dataset. This dataset has 500,000 images of bins containing one or more objects present in it. Corresponding to each image, is a metadata file, which contains information about the image, like the number of objects it has, the dimensions and type of objects. For our problem statement, we only need the total count of objects in the image. 

To understand more about the Data, see `Create_Data_Capstone.ipynb`

### Access
Use `Create_Data_Capstone.ipynb` to create Dataset for Model Training, and upload in S3 through Web Browser or CLI

## Model Training
`Final_Model_Train_Inference.ipynb` details this process and shows how we beat the Benchmark

### We chose a Resnet50 Pre-Trained Model, and did Hyper Parameter Searching to refine it further and further
### The Best Model we had was with HP: Batch Size 32, Learning Rate: 0.002, trained for 10 Epochs
### The details are in the Capstone Report and Final Model Training Notebook

## Machine Learning Pipeline

### The Project Jupyter Notebooks are very well documented with Markdowns to guide you. 
### Following the below sequence will help understand the pipeline and implement the project on your own:

1. **Data Preparation and Analysis:** `Create_Data_Capstone.ipynb` documents this process and has the analysis results
2. **Creating Benchmarks for Project:** `Benchmark_Model.ipynb` documents how the Benchmarks were chosen and created in SageMaker
3. **Training different Models:** `Final_Model_Train_Inference.ipynb` documents how different Models were trained to beat Benchmark
4. **Model Evaluation and Deployment:**`Final_Model_Train_Inference.ipynb` documents the evaluation and deployment on SageMaker


### Benchmark vs. Best Model

| **Class** | **F1 for Benchmark** | **F1 for Best Model** |
|-----------|----------------------|-----------------------|
| **1**     | 0.575                | 0.663                 |
| **2**     | 0.258                | 0.302                 |
| **3**     | 0.151                | 0.180                 |
| **4**     | 0.150                | 0.281                 |
| **5**     | 0.412                | 0.483                 |


| **Accuracy for Benchmark** | **Accuracy for Best Model** | **% Increase in Accuracy** |
|----------------------------|-----------------------------|----------------------------|
| 0.354 = 35.4%              | 0.4 = 40%                   | 0.046 = 4.6%               |



## Standout Suggestions

1. HP Tuning
2. Model Deployment and Inference
3. Debugger and Profiler Report
4. Created a custom script for Balancing Classes by fetching additional data (`Get_More_Data_Script.ipynb`)
